<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Ashma Yonghang | Variational Inference</title>
    <meta name="author" content="Ashma Yonghang" />
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://ayonghan.github.io/news/announcement_2/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://ayonghan.github.io/"><span class="font-weight-bold">Ashma</span>   Yonghang</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Variational Inference</h1>
    <p class="post-meta">May 11, 2024</p>
    <p class="post-tags">
      <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>

    </p>
  </header>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#prior-distribution" aria-label="Prior Distribution">Prior Distribution</a><ul>
                        
                    <li>
                        <a href="#joint-distribution" aria-label="Joint Distribution">Joint Distribution</a></li>
                    <li>
                        <a href="#marginal-likelihood" aria-label="Marginal Likelihood">Marginal Likelihood</a></li></ul>
                </li>

          
                <li>
                    <a href="#importance-sampling" aria-label="Importance sampling">Importance sampling</a>
                </li>

                <li>
                    <a href="#variational-posterior" aria-label="Variational Posterior">Variational Posterior</a>
                </li>

                <li>
                      <a href="#tractability" aria-label="Tractability">Tractability</a><ul>
                        
                    <li>
                        <a href="#why-q-is-easier-to-tract" aria-label="Why q is easier to tract?">Why q is easier to tract?</a><ul>
                            <li>
                            <a href="#P-when-$s_t$-is-continuous" aria-label=>P when $s_t$ is continuous">P when $s_t$ is continuous</a></li>
                            <li>
                                <a href="#p-when-$s_t$-is-discrete" aria-label="P when $s_t$ is discrete">P when $s_t$ is discrete</a></li></ul></li>
                    <li>
                        <a href="#why-p-is-difficult-to-tract" aria-label="Reward Function in RL">Reward Function in RL</a></li>
                    <li>
                        <a href="#this-is-why" aria-label="This is why">This is why</a></li></ul>
                </li>

              <li>
                    <a href="#jensen’s-inequality" aria-label="Jensen’s Inequality">Jensen’s Inequality</a>
              </li>

              <li>
                    <a href="#deriving-the-variational-bound-elbo" aria-label="Deriving the Variational Bound ELBO">Deriving the Variational Bound ELBO</a>
              </li>

              <li>
                    <a href="#non-differentiability-problem" aria-label="Non-differentiability Problem: Stochastic Sampling from variational posterior">Non-differentiability Problem: Stochastic Sampling from variational posterior</a>
              </li>

              <li>
                    <a href="#solution-reparameterization-trick" aria-label="Solution: Reparameterization Trick">Solution: Reparameterization Trick</a>
              </li>        
          
          </ul>
        </div>
    </details>
</div>

  <article class="post-content">
    <p>Mathematics of Evidence Lower Bound (ELBO) objective formulation is inevitably encountered when dealing with latent dynamics models and VAEs. In this blog, we break it down along with necessary concepts like the reparameterization trick, importance sampling and Jensen's inequality.</p>
    <h2 id="prior-distribution">Prior Distribution<a hidden class="anchor" aria-hidden="true" href="#prior-distribution">#</a></h2>
    <p>Latent variable models aim to model observed data  $o_{1:T}$ while leveraging hidden (latent) variables $s_{1:T}$​.</p>
    <h4 id="joint-distribution">Joint Distribution<a hidden class="anchor" aria-hidden="true" href="#joint-distribution">#</a></h4>
    <p> In these models, the joint distribution can be defined as:
    $$ p(o_{1:T}, s_{1:T} \mid a_{1:T}) = \prod_{t=1}^T p(o_t \mid s_t) p(s_t \mid s_{t-1}, a_{t-1}) \tag{1} \label{eq:jointlikelihood}$$
    Where $p(o_t \mid s_t)$ is the likelihood of observing $o_t$ given the latent state $s_t$​ and $p(s_t \mid s_{t-1}, a_{t-1})$ is transition dynamics of latent states.
    </p>

    <h4 id="marginal-likelihood">Marginal Likelihood<a hidden class="anchor" aria-hidden="true" href="#marginal-likelihood">#</a></h4>
    <p>To compute the marginal likelihood, we marginalize out $s_{1:T}$ as the following equation:
    $$ p(o_{1:T} \mid a_{1:T}) = \int p(o_{1:T}, s_{1:T} \mid a_{1:T}) \, ds_{1:T} \tag{2} \label{eq:marginallikelihood} $$
    This integral is typically intractable due to high dimensionality and complex dependencies between variables. So we instead approximate this intractable data likelihood distribution with a tractable posterior approximation.</p>
        
    <h2 id="importance-sampling">Importance sampling<a hidden class="anchor" aria-hidden="true" href="#importance-sampling">#</a></h2>
    <p>This statistical technique used to approximate expectations or probabilities by sampling from an easier-to-handle distribution (called the importance distribution) q(x) rather than the true, often intractable, distribution p(x). While sampling q(x)  reweight the samples to account for the difference between q(x) and p(x), so it also known as importance weighting.
    $$ \mathbb{E}_{p(x)}[f(x)] = \int f(x) p(x) \, dx  $$
    Rewriting using q(x), we multiply and divide by q(x):
    $$ \mathbb{E}_{p(x)}[f(x)] = \int f(x) \frac{p(x)}{q(x)} q(x) \, dx = \mathbb{E}_{q(x)} \left[ f(x) \frac{p(x)}{q(x)} \right] $$
    
    Where w(x)= $\frac{p(x)}{q(x)}$​ are the importance weights.</p>

    <h2 id="variational-posterior">Variational Posterior q<a hidden class="anchor" aria-hidden="true" href="#variational-posterior">#</a></h2>
    <p>Variational posterior approximating the true posterior distribution $p(s_{1:T} \mid o_{1:T}, a_{1:T})$ is expressed as:
    $$q(s_{1:T} \mid o_{1:T}, a_{1:T}) = \prod_{t=1}^T q(s_t \mid o_t, a_{\lt t})$$ 
    </p>
      
    <h2 id="tractability">Tractability<a hidden class="anchor" aria-hidden="true" href="#tractability">#</a></h2>
    <h3 id="why-q-is-easier-to-tract">Why q is easier to tract?<a hidden class="anchor" aria-hidden="true" href="#why-q-is-easier-to-tract">#</a></h3>
    <p>This factorization assumes that each latent variable $s_t​$ depends only on local observations $o_t$​ and preceding actions $a_{\lt t}$​, rather than the entire sequence of states. In the marginal likelihood or prior distribution, as in equation (\ref{eq:marginallikelihood}), one has to do the integral over all possible state values (probabilities of all states leading to an observation from $o_{1:T}$), which is infinite as the state value is continuous.
    </p>
    <h3 id="why-p-is-difficult-to-tract">Why p is difficult to tract?<a hidden class="anchor" aria-hidden="true" href="#why-p-is-difficult-to-tract">#</a></h3>
    <h4 id="P-when-$s_t$-is-continuous">P when $s_t$ is continuous<a hidden class="anchor" aria-hidden="true" href="#P-when-$s_t$-is-continuous">#</a></h4>
    <p>Equation (\ref{eq:marginallikelihood}) involves integrating over $s_{1:T}$​, which consists of the T latent variables $s_1, s_2, \dots, s_T$​.
    $$\int ds_{1:T} = \int_{\mathbb{R}^n} ds_1 \int_{\mathbb{R}^n} ds_2 \cdots \int_{\mathbb{R}^n} ds_T$$
    
    The sequence $s_{1:T}$​ has a finite length T, but The integral aggregates over all possible continuous values of $s_1, s_2, \dots$ leading to a continuous T-fold integral.
    </p>
      
    <h4 id="p-when-$s_t$-is-discrete">P when $s_t$ is discrete<a hidden class="anchor" aria-hidden="true" href="#p-when-$s_t$-is-discrete">#</a></h4>
    <p>If $s_t$​ takes on discrete values $s_t^{(1)}, s_t^{(2)}, \dots, s_t^{(K)}$, where K is the number of discrete states for $s_t$​, the marginal likelihood becomes:
    $$.\sum_{s_{1:T}} = \sum_{s_1} \sum_{s_2} \cdots \sum_{s_T}$$
    Each summation is over all possible values of $s_t$​. For categorical $s_t$​ might be with K states and T variables, we need to do a sum over $K^T$ combinations of $s_{1:T}$.
    <i>Note:</i> In discrete models (e.g., Hidden Markov Models), the marginal likelihood summation can often be computed efficiently using algorithms like the forward algorithm, avoiding an explicit sum over all $K^T$ possibilities.</p>

    <h4 id="this-is-why">This is why<a hidden class="anchor" aria-hidden="true" href="#this-is-why">#</a></h4>
    <p>Marginal P is computationally intractable as exact integration over continuous spaces or $K^T$ summations over high dimensional discrete state with K values is rarely feasible. Computation of marginal likelihood using analytical methods is not possible, numerical methods are needed. 
    <i>Note:</i> This is why techniques like variational inference or Monte Carlo sampling are used to approximate the marginal likelihood.
    Before proceeding with understanding expressions of ELBO, apart from about importance sampling, we will also need to know Jensen's equality.
    </p>

    <h2 id="jensen’s-inequality">Jensen’s Inequality<a hidden class="anchor" aria-hidden="true" href="#jensen’s-inequality">#</a></h2>
    <p>Jensen's inequality states that for a convex (or concave) function f(x), and a random variable X:
    $$f(\mathbb{E}[X]) \leq \mathbb{E}$$
    If f(x) is concave, the inequality is flipped: 
    $$ \ln \mathbb{E}[x] \geq \mathbb{E}[\ln x] $$
    </p>

    <h2 id="deriving-the-variational-bound-elbo">Deriving the Variational Bound ELBO<a hidden class="anchor" aria-hidden="true" href="#deriving-the-variational-bound-elbo">#</a></h2>
    <p>In equation (\ref{eq:marginallikelihood}), we introduce a variational approximation $q(s_{1:T} \mid o_{1:T}, a_{1:T})$ to the true posterior $p(s_{1:T} \mid o_{1:T}, a_{1:T})$ using importance sampling:

    $$ p(o_{1:T} \mid a_{1:T}) = \int q(s_{1:T} \mid o_{1:T}, a_{1:T}) \frac{p(o_{1:T}, s_{1:T} \mid a_{1:T})}{q(s_{1:T} \mid o_{1:T}, a_{1:T})} \, ds_{1:T} $$
    Taking the logarithm:
    $$ \ln p(o_{1:T} \mid a_{1:T}) = \ln \int q(s_{1:T} \mid o_{1:T}, a_{1:T}) \frac{p(o_{1:T}, s_{1:T} \mid a_{1:T})}{q(s_{1:T} \mid o_{1:T}, a_{1:T})} \, ds_{1:T} $$
    Since the logarithm is concave, we can apply Jensen's inequality as:
    $$\ln(\mathbb{E}[X]) \geq \mathbb{E}[\ln(X)]$$ so that
    $$ \ln p(o_{1:T} \mid a_{1:T}) \geq \int q(s_{1:T} \mid o_{1:T}, a_{1:T}) \ln \frac{p(o_{1:T}, s_{1:T} \mid a_{1:T})}{q(s_{1:T} \mid o_{1:T}, a_{1:T})} \, ds_{1:T} $$
    This forms the Evidence Lower Bound (ELBO):
    $$
    \ln p(o_{1:T} \mid a_{1:T}) \geq \mathbb{E}_{q(s_{1:T} \mid o_{1:T}, a_{1:T})} 
    \left[ \ln p(o_{1:T}, s_{1:T} \mid a_{1:T}) - \ln q(s_{1:T} \mid o_{1:T}, a_{1:T}) \right]
    $$  
    Equation of joint likelihood:
    $$ p(o_{1:T}, s_{1:T} \mid a_{1:T}) = \prod_{t=1}^T p(o_t \mid s_t) p(s_t \mid s_{t-1}, a_{t-1}) $$
    Using equation (\ref{eq:jointlikelihood}) of joint likelihood into this ELBO:
    $$\ln p(o_{1:T} \mid a_{1:T}) \geq \mathbb{E}_{q(s_{1:T} \mid o_{1:T}, a_{1:T})} \left[ \sum_{t=1}^T \ln p(o_t \mid s_t) + \ln p(s_t \mid s_{t-1}, a_{t-1}) - \ln q(s_t \mid o_t, a_{\lt t}) \right].$$</p>

    <h2 id="elbo-components">ELBO Components<a hidden class="anchor" aria-hidden="true" href="#elbo-components">#</a></h2>
    <p>We can now split the ELBO into two main components: Reconstruction Term:
    measures how well the model reconstructs the observations given the latent states and KL Divergence measures how much the variational posterior $q(s_t \mid o_t, a_{\;t t})$ deviates from the prior dynamics $p(s_t \mid s_{t-1}, a_{t-1})$.
    Final ELBO can be written as:
    $$ \mathcal{L} = \underbrace{\sum_{t=1}^T \mathbb{E}_{q(s_t \mid o_t)} [\ln p(o_t \mid s_t)]}_{\text{Reconstruction Loss}} - \underbrace{\sum_{t=1}^T D_{\mathrm{KL}} \left( q(s_t \mid o_t, a_{\lt t}) \parallel p(s_t \mid s_{t-1}, a_{t-1}) \right)}_{\text{Complexity}} $$
    The loss function becomes:
    $$\mathcal{L} = -\text{ELBO} = \text{Reconstruction Loss} + \text{Complexity Penalty}$$
    </p>

    <h2 id="non-differentiability-problem">Non-differentiability Problem: Stochastic Sampling from variational posterior<a hidden class="anchor" aria-hidden="true" href="#non-differentiability-problem">#</a></h2>
    <p>While maximizing the ELBO if q is a complex function of parameters (e.g., neural network outputs), gradients for optimization can become problematic due to the non-differentiable nature of sampling operations in q. 
    How? To optimize the parameters $\phi$ of $q(s_{1:T} \mid o_{1:T}, a_{1:T})$, we compute gradients of \mathcal{L} with respect to $\phi$. The expectation is over $q(s_{1:T} \mid o_{1:T}, a_{1:T}))$, meaning we must sample $s_{1:T}$​ to estimate this expectation. But sampling $s_{1:T} \sim q(s_{1:T} \mid o_{1:T}, a_{1:T})$ is stochastic and hence non-differentiable - the stochasticity in sampling disrupts the computation of gradients via backpropagation.
    </p>

    <h2 id="solution-reparameterization-trick">Solution: Reparameterization Trick<a hidden class="anchor" aria-hidden="true" href="#solution-reparameterization-trick">#</a></h2>
    <p>It re-expresses the stochastic sampling as a deterministic transformation of a fixed random variable. Instead of directly sampling $s_t$​ from $q(s_t \mid o_t, a_{\lt t})$, It introduces a Noise $\epsilon$ by sampling from a fixed distribution (e.g., $\mathcal{N}(0, 1)$ - we move the randomness to $\epsilon$ that is independent of learnable parameters $\phi$ making the computation of $\nabla_\phi$ tractable.
    In the deterministic transformation as below gradients $\nabla_\phi$​ can flow through $\mu_\phi$​ and $\sigma_\phi$​ via backpropagation:
    $$s_t = \mu_\phi + \sigma_\phi \cdot \epsilon \sim \mathcal{N}(0, 1)$$
    </p>
    
        


  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2022 Ashma  Yonghang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true,
      tags: 'all' 
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
  <script defer type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>
